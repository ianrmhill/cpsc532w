{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OS imports\n",
    "import logging\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import torch as tc\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, multivariate_normal\n",
    "from scipy.optimize import curve_fit\n",
    "import wandb\n",
    "\n",
    "# Project imports\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.distributions.constraints as constraints\n",
    "from pyro.infer import Predictive, Importance, EmpiricalMarginal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights & biases\n",
    "# NOTE: Only turn this on when you think you have got everything working\n",
    "wandb_run = False\n",
    "if wandb_run:\n",
    "    wandb.init(project='HW1-Q5', entity='cs532-2022')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian linear regression\n",
    "```\n",
    "(defn observe-data [_ data slope bias]\n",
    "  (let [xn (first data)\n",
    "        yn (second data)\n",
    "        zn (+ (* slope xn) bias)]\n",
    "    (observe (normal zn 1.0) yn)\n",
    "    (rest (rest data))))\n",
    "(let [slope (sample (normal 0.0 10.0))\n",
    "      bias  (sample (normal 0.0 10.0))\n",
    "      data  (vector 1.0 2.1 2.0 3.9 3.0 5.3\n",
    "                   4.0 7.7 5.0 10.2 6.0 12.9)]\n",
    "  (loop 6 data observe-data slope bias)\n",
    "  (vector slope bias))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pyro settings\n",
    "pyro.enable_validation(True)\n",
    "pyro.set_rng_seed(1)\n",
    "logging.basicConfig(format='%(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data and parameters\n",
    "# NOTE: You need to fill this in\n",
    "x_numpy = None\n",
    "y_numpy = None\n",
    "x_torch = tc.tensor(x_numpy)\n",
    "y_torch = tc.tensor(y_numpy)\n",
    "sigma = None\n",
    "alpha = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check model fitting (non Bayesian) with scipy\n",
    "def f(x, bias, slope):\n",
    "    return bias+x*slope\n",
    "popt, pcov = curve_fit(f, x_numpy, y_numpy)\n",
    "print('Best-fitting parameters:', popt)\n",
    "print('Parameter covariance matrix:\\n', pcov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data\n",
    "plt.plot(x_numpy, y_numpy, marker='o', ls='None')\n",
    "plt.xlabel('x')\n",
    "plt.xlim(left=0.)\n",
    "plt.ylabel('y')\n",
    "plt.ylim(bottom=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilistic model\n",
    "def model(x, y):\n",
    "    # NOTE: You need to fill this in\n",
    "    return None\n",
    "\n",
    "# Make a plot of the graph\n",
    "pyro.render_model(model, model_args=(x_torch, y_torch), render_distributions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Clear pyro to begin training\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# These should be reset each training loop\n",
    "# NOTE: You need to fill this in\n",
    "guide = None\n",
    "adam = None\n",
    "elbo = None\n",
    "svi = None\n",
    "steps = None\n",
    "\n",
    "# Calculate the losses\n",
    "losses = []\n",
    "for step in range(steps): \n",
    "    loss = svi.step(x_torch, y_torch)\n",
    "    losses.append(loss)\n",
    "    if step%1000 == 0:\n",
    "        logging.info('Elbo loss: {}'.format(loss))\n",
    "        if wandb_run: wandb.log({'loss': loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curve\n",
    "plt.subplots(figsize=(12, 4))\n",
    "for i in range(2):\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel('Stochastic Variational Inference (SVI) step')\n",
    "    plt.ylabel('Evidence Lower BOund (ELBO) loss')\n",
    "    plt.ylim(bottom=0.) if i==0 else plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at inferred parameters\n",
    "for name, value in pyro.get_param_store().items():\n",
    "    print(name, pyro.param(name).data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from variational inference distributions\n",
    "nsamples = int(1e3)\n",
    "with pyro.plate('samples', nsamples, dim=-1):\n",
    "    samples = guide(x_torch)\n",
    "if wandb_run:\n",
    "    for bias, slope in zip(samples['bias'], samples['slope']):\n",
    "        wandb.log({'bias': bias, 'slope': slope})\n",
    "bias = samples['bias'].detach().numpy() # NOTE: Convert to numpy here\n",
    "slope = samples['slope'].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for plotting\n",
    "\n",
    "def analytical_covariance(x, y, sigma, alpha, verbose=False):\n",
    "    # Analytical expression for the covariance\n",
    "    # NOTE: From the previous questions you should be able to fill this in\n",
    "    return None\n",
    "\n",
    "def analytical_mean(x, y, sigma, alpha, verbose=False):\n",
    "    # Analytical expression for the mean\n",
    "    # NOTE: From the previous questions you should be able to fill this in\n",
    "    return None\n",
    "\n",
    "def get_distribution(plt, mu, sig, N=200):\n",
    "    # Distribution for 1D marginals\n",
    "    xlim = plt.gca().get_xlim()\n",
    "    x = np.linspace(xlim[0], xlim[1], N)\n",
    "    rv = norm(mu, sig)\n",
    "    f = rv.pdf(x)\n",
    "    return x, f\n",
    "\n",
    "def get_contours(plt, mu, cov, N=200):\n",
    "    # Contours for 2D posterior\n",
    "    xlim = plt.gca().get_xlim(); ylim = plt.gca().get_ylim()\n",
    "    X = np.linspace(xlim[0], xlim[1], N)\n",
    "    Y = np.linspace(ylim[0], ylim[1], N)\n",
    "    X, Y = np.meshgrid(X, Y); pos = np.dstack((X, Y))\n",
    "    rv = multivariate_normal(mu, cov)\n",
    "    Z = rv.pdf(pos)\n",
    "    return X, Y, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialize\n",
    "plot_analytical = False # NOTE: Turn this on if you figure out what the analytical result is\n",
    "plt.subplots(figsize=(7, 7))\n",
    "if plot_analytical:\n",
    "    mu = analytical_mean(x_numpy, y_numpy, sigma, alpha)\n",
    "    cov = analytical_covariance(y_numpy, sigma, alpha)\n",
    "\n",
    "# Slope histogram\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.hist(slope, density=True, bins='auto', alpha=0.7)\n",
    "plt.axvline(slope.mean(), color='black', ls='--', label='Mean: %1.2f'%slope.mean())\n",
    "plt.axvline(slope.mean()-slope.std(), color='black', ls=':', label='Std: %1.2f'%slope.std())\n",
    "plt.axvline(slope.mean()+slope.std(), color='black', ls=':')\n",
    "if plot_analytical:\n",
    "    x, f = get_distribution(plt, mu[0], np.sqrt(cov[0, 0]))\n",
    "    plt.plot(x, f, color='black', label='analytical')\n",
    "plt.gca().set_xticklabels([])\n",
    "plt.yticks([])\n",
    "plt.legend()\n",
    "\n",
    "# Bias histogram\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.hist(bias, density=True, bins='auto', alpha=0.7)\n",
    "plt.axvline(bias.mean(), color='black', ls='--', label='Mean: %1.2f'%bias.mean())\n",
    "plt.axvline(bias.mean()-bias.std(), color='black', ls=':', label='Std: %1.2f'%bias.std())\n",
    "plt.axvline(bias.mean()+bias.std(), color='black', ls=':')\n",
    "if plot_analytical:\n",
    "    x, f = get_distribution(plt, mu[1], np.sqrt(cov[1, 1]))\n",
    "    plt.plot(x, f, color='black', label='analytical')\n",
    "plt.xlabel('bias')\n",
    "plt.yticks([])\n",
    "plt.legend()\n",
    "\n",
    "# Slope vs. bias scatter\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.scatter(slope, bias, alpha=0.2)\n",
    "if plot_analytical:\n",
    "    X, Y, Z = get_contours(plt, mu, cov)\n",
    "    plt.contour(X, Y, Z, colors='black', alpha=0.9, levels=3)\n",
    "plt.xlabel('slope')\n",
    "plt.ylabel('bias')\n",
    "\n",
    "# Finalize\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the covariance and correlation matrices of the samples\n",
    "cov = np.cov(np.stack([bias, slope]))\n",
    "print('Covariance matrix:\\n', cov)\n",
    "r = np.corrcoef(bias, slope)\n",
    "print('Correlation matrix:\\n', r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior predictive\n",
    "x_new = tc.tensor([0.])\n",
    "n = int(1e3)\n",
    "\n",
    "# Calculate the posterior predictive at the new data point\n",
    "ys_new = []\n",
    "for _ in range(n):\n",
    "     # NOTE: You need to fill this in\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot posterior predictive\n",
    "plt.hist(ys_new, bins='auto', density=True, alpha=0.7)\n",
    "mean = np.mean(ys_new); std = np.std(ys_new)\n",
    "plt.axvline(mean, color='black', ls='--', label='Mean: %1.2f'%(mean))\n",
    "plt.axvline(mean-std, color='black', ls=':', label='Std: %1.2f'%(std))\n",
    "plt.axvline(mean+std, color='black', ls=':')\n",
    "plt.xlabel('Posterior predictive at new datum')\n",
    "plt.yticks([])\n",
    "plt.legend()\n",
    "if wandb_run: wandb.log({'Question 5; predictive at new datum': wandb.Image(plt)})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression model\n",
    "def regression(x, slope, bias):\n",
    "    # NOTE: Fill this in\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for posterior predictive\n",
    "xmin = 0.; xmax = 7.; nx = 17\n",
    "x = np.linspace(xmin, xmax, nx)\n",
    "ys = []\n",
    "for s, b in zip(slope, bias):\n",
    "    y = regression(x, s, b)\n",
    "    ys.append(y)\n",
    "ys = np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot full posterior predictive\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.fill_between(x, np.percentile(ys, 2.28, axis=0), np.percentile(ys, 97.72, axis=0), color='C0', alpha=0.25) # 2-sigma\n",
    "plt.fill_between(x, np.percentile(ys, 15.87, axis=0), np.percentile(ys, 84.13, axis=0), color='C0', alpha=0.5) # 1-sigma\n",
    "plt.plot(x, ys.mean(axis=0), color='C0', label='Model predictions')\n",
    "plt.scatter(x_numpy, y_numpy, marker='o', color='black', label='Training data')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "if wandb_run: wandb.log({'Question 5; predictive': wandb.Image(plt)})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalise weights & biases\n",
    "if wandb_run:\n",
    "    data = [[j, s, b] for j, (s, b) in enumerate(zip(slope, bias))]\n",
    "    table = wandb.Table(data=data, columns=['sample', 'slope', 'bias'])\n",
    "    wandb_log = {}\n",
    "    wandb_log['Question 5; slope'] = wandb.plot.histogram(table, value='slope', title='Question 5; slope')\n",
    "    wandb_log['Question 5; bias'] = wandb.plot.histogram(table, value='bias', title='Question 5; bias')\n",
    "    wandb_log['Question 5; scatter'] = wandb.plot.scatter(table, x='slope', y='bias', title='Question 5; slope vs. bias')\n",
    "    wandb.log(wandb_log)\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "532w",
   "language": "python",
   "name": "532w"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e5b776bc967e1a56f4d4f90cfeff61fdd27944f0bb9f84040c3865cfa6d4e360"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
